{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: **Assignment 2 - COSC3013 Computational Machine Learning - End-to-end Machine Learning Project**\n",
    "\n",
    "Student ID: **S3979613**\n",
    "\n",
    "Student Name and email (contact info): **Dao Sy Trung Kien - S3979613@rmit.edu.vn**\n",
    "\n",
    "Affiliations: **RMIT University Vietnam.**\n",
    "\n",
    "Date of Report: 03/08/2023\n",
    "\n",
    "I certify that this is all my own original work. If I took any parts from elsewhere, then they were non-essential parts of the assignment, and they are clearly attributed in my submission.  I will show I agree to this honor code by typing \"Yes\": Yes.\n",
    "\n",
    "Please start your report here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages - Pandas, Numpy, Seaborn, Scipy, Impute\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "import matplotlib.style as style; style.use('fivethirtyeight')\n",
    "np.random.seed(0)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelling\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data and check for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for import data from Paitients_Files_Train csv file\n",
    "# df_train = pd.read_csv(r'C:\\Users\\Kien\\Downloads\\Computational ML\\UCI-electricity\\UCI-electricity\\UCI_data.csv')\n",
    "df_train = pd.read_csv(r'UCI_data.csv')\n",
    "# Print out data.\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Missing Values % contribution in Train Data\n",
    "df_train_null = round(100*(df_train.isnull().sum())/len(df_train), 2)\n",
    "df_train_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of the Training dataset\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info of the dataframe columns\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate Z-scores for each column\n",
    "z_scores = stats.zscore(df_train.select_dtypes(include=[float, int]))\n",
    "\n",
    "# Create a boolean mask for rows where all Z-scores are less than 3\n",
    "# mask = (abs(z_scores) < 3).all(axis=1)\n",
    "\n",
    "# Filter the DataFrame using the mask\n",
    "# df_train_filtered = df_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_filtered = df_train\n",
    "df_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median of the Windspeed column\n",
    "median_windspeed = df_train_filtered['Windspeed'].median()\n",
    "# Replace missing values with median\n",
    "df_train_filtered['Windspeed'].replace(0, median_windspeed, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datetime column to a pandas datetime object\n",
    "df_train_filtered['datetime'] = pd.to_datetime(df_train_filtered['date'])\n",
    "\n",
    "# Create new columns for date and time\n",
    "df_train_filtered['date'] = df_train_filtered['datetime'].dt.date\n",
    "df_train_filtered['time'] = df_train_filtered['datetime'].dt.time\n",
    "\n",
    "#Hour of the Day\n",
    "df_train_filtered['hour'] = df_train_filtered['datetime'].dt.hour\n",
    "\n",
    "# Day Part\n",
    "df_train_filtered['day_part'] = pd.cut(df_train_filtered['hour'], bins=[0, 6, 12, 18, 24], labels=['Night', 'Morning', 'Afternoon', 'Evening'], right=False)\n",
    "\n",
    "# Season\n",
    "df_train_filtered['month'] = df_train_filtered['datetime'].dt.month\n",
    "df_train_filtered['season'] = df_train_filtered['month'].apply(lambda x: 'Winter' if x in [12, 1, 2] else 'Spring' if x in [3, 4, 5] else 'Summer' if x in [6, 7, 8] else 'Fall')\n",
    "\n",
    "# Drop the original datetime column if no longer needed\n",
    "df_train_filtered.drop('datetime', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'day_part' categorical feature into numerical values\n",
    "df_train_filtered['day_part_encoded'] = df_train_filtered['day_part'].cat.codes\n",
    "df_train_filtered['day_part_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_columns = ['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9']\n",
    "df_train_filtered['temperature_mean'] = df_train_filtered[temperature_columns].mean(axis=1)\n",
    "df_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humidity_columns = ['RH_1', 'RH_2', 'RH_3', 'RH_4', 'RH_5', 'RH_6', 'RH_7', 'RH_8', 'RH_9']\n",
    "df_train_filtered['humidity_mean'] = df_train_filtered[humidity_columns].mean(axis=1)\n",
    "df_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_filtered['daylight'] = df_train_filtered['hour'].between(6,18)\n",
    "df_train_filtered['daylight'] = np.where(df_train_filtered['daylight'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need create hot day/ cold day "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "numeric_df_train = df_train_filtered.select_dtypes(include=[np.number])\n",
    "plt.figure(figsize=(30, 25))\n",
    "sns.heatmap(numeric_df_train.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df_train = df_train_filtered.drop(columns=['day_part', 'time', 'season','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "numeric_df_train.boxplot()\n",
    "plt.title('Box plot of numerical features after handling outliers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-scores for each numerical feature\n",
    "z_scores = np.abs(stats.zscore(numerical_df_train))\n",
    "outliers = np.where(z_scores > 3)  # Identify points more than 3 standard deviations away\n",
    "# Calculate IQR(measure of data statistical dispersion) for each numerical feature\n",
    "Q1 = numerical_df_train.quantile(0.25)\n",
    "Q3 = numerical_df_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = ((numerical_df_train < (lower_bound)) | (numerical_df_train > (upper_bound))).sum()\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap and floor the outliers\n",
    "for feature in numerical_df_train:\n",
    "    lower_cap = Q1[feature] - 1.5 * IQR[feature]\n",
    "    upper_cap = Q3[feature] + 1.5 * IQR[feature]\n",
    "    numerical_df_train[feature] = np.where(numerical_df_train[feature] < lower_cap, lower_cap, numerical_df_train[feature])\n",
    "    numerical_df_train[feature] = np.where(numerical_df_train[feature] > upper_cap, upper_cap, numerical_df_train[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "numerical_df_train.boxplot()\n",
    "plt.title('Box plot of numerical features')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Fitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "numeric_df_train = df_train_filtered.select_dtypes(include=[np.number])\n",
    "plt.figure(figsize=(30, 25))\n",
    "sns.heatmap(numeric_df_train.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training, and test sets (70% train, 30% test)\n",
    "target_feature = numerical_df_train['TARGET_energy']\n",
    "features = numerical_df_train.drop(columns=['TARGET_energy'],axis=1)\n",
    "selected_features= ['hour','day_part_encoded','daylight']\n",
    "X_train, X_test, y_train, y_test = train_test_split(features[selected_features], target_feature, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# # Define the models and their hyperparameter grids\n",
    "models = {\n",
    "    'LinearRegression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {  \n",
    "            'fit_intercept':[True , False], \n",
    "            'n_jobs':[None, -1, 1],            \n",
    "            'positive':[False, True]    \n",
    "        }       \n",
    "    },\n",
    "    'Ridge': {\n",
    "        'model': Ridge(random_state=42),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1, 10, 100]\n",
    "        }\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model': Lasso(random_state=42),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1, 10, 100]\n",
    "        }\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'model': ElasticNet(),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "            'l1_ratio': [0.1, 0.5, 0.7, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'model': DecisionTreeRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'epsilon': [0.01, 0.1, 0.2],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'model': AdaBoostRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2]\n",
    "        }\n",
    "    },\n",
    "     'XGBoost': {\n",
    "        'model': xgb.XGBRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': lgb.LGBMRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'model': MLPRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': ['adam', 'lbfgs'],\n",
    "            'learning_rate_init': [0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# for model_name, model in models.items():\n",
    "#     model.fit(X_train, y_train)\n",
    "#     # Make predictions\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     # Evaluate the model\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     r2 = r2_score(y_test, y_pred)\n",
    "#     print(f\"{model} - MSE: {mse}, R²: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "LinearRegression best estimator: LinearRegression(positive=True)\n",
      "LinearRegression best params: {'fit_intercept': True, 'n_jobs': None, 'positive': True}\n",
      "LinearRegression best score: -29.6687\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Ridge best estimator: Ridge(alpha=100, random_state=42)\n",
      "Ridge best params: {'alpha': 100}\n",
      "Ridge best score: -29.6719\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Lasso best estimator: Lasso(alpha=0.1, random_state=42)\n",
      "Lasso best params: {'alpha': 0.1}\n",
      "Lasso best score: -29.6650\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "ElasticNet best estimator: ElasticNet(alpha=0.1, l1_ratio=1.0)\n",
      "ElasticNet best params: {'alpha': 0.1, 'l1_ratio': 1.0}\n",
      "ElasticNet best score: -29.6650\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "DecisionTreeRegressor best estimator: DecisionTreeRegressor(random_state=42)\n",
      "DecisionTreeRegressor best params: {'max_depth': None, 'min_samples_split': 2}\n",
      "DecisionTreeRegressor best score: -26.9732\n",
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n",
      "RandomForestRegressor best estimator: RandomForestRegressor(max_depth=10, n_estimators=200, random_state=42)\n",
      "RandomForestRegressor best params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "RandomForestRegressor best score: -26.9775\n",
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "SVR best estimator: SVR(C=10, epsilon=0.01)\n",
      "SVR best params: {'C': 10, 'epsilon': 0.01, 'kernel': 'rbf'}\n",
      "SVR best score: -26.1488\n",
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n",
      "GradientBoostingRegressor best estimator: GradientBoostingRegressor(n_estimators=50, random_state=42)\n",
      "GradientBoostingRegressor best params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "GradientBoostingRegressor best score: -26.9621\n",
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "KNeighborsRegressor best estimator: KNeighborsRegressor(n_neighbors=9)\n",
      "KNeighborsRegressor best params: {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "KNeighborsRegressor best score: -29.4669\n",
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "AdaBoostRegressor best estimator: AdaBoostRegressor(learning_rate=0.01, random_state=42)\n",
      "AdaBoostRegressor best params: {'learning_rate': 0.01, 'n_estimators': 50}\n",
      "AdaBoostRegressor best score: -27.4696\n",
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "XGBoost best estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=42, ...)\n",
      "XGBoost best params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "XGBoost best score: -26.9712\n",
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33\n",
      "[LightGBM] [Info] Number of data points in the train set: 13814, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 81.874548\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM best estimator: LGBMRegressor(max_depth=3, random_state=42)\n",
      "LightGBM best params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "LightGBM best score: -26.9713\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "MLPRegressor best estimator: MLPRegressor(activation='tanh', hidden_layer_sizes=(50, 50), random_state=42,\n",
      "             solver='lbfgs')\n",
      "MLPRegressor best params: {'activation': 'tanh', 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'solver': 'lbfgs'}\n",
      "MLPRegressor best score: -26.9654\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search for each model fitting on X_train, y_train\n",
    "best_params = {}\n",
    "best_scores = {}\n",
    "best_models = {}\n",
    "skf = StratifiedKFold(n_splits = 10)\n",
    "for model_detail in models:\n",
    "    model = models[model_detail]['model']\n",
    "    param_grid = models[model_detail]['params']\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=skf, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params[model_detail] = grid_search.best_params_\n",
    "    best_scores[model_detail] = grid_search.best_score_\n",
    "    print(f'{model_detail} best params: {grid_search.best_params_}')\n",
    "    print(f'{model_detail} best score: {grid_search.best_score_:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE for LinearRegression(positive=True): 1540.6557935151873\n",
      "Test MAE for LinearRegression(positive=True): 29.81502703435183\n",
      "Test MPE for LinearRegression(positive=True): 0.4380858874907701\n",
      "\n",
      "Test MSE for Ridge(alpha=100, random_state=42): 1539.1668992186985\n",
      "Test MAE for Ridge(alpha=100, random_state=42): 29.814473660429147\n",
      "Test MPE for Ridge(alpha=100, random_state=42): 0.4382421890023142\n",
      "\n",
      "Test MSE for Lasso(alpha=0.1, random_state=42): 1539.831647666405\n",
      "Test MAE for Lasso(alpha=0.1, random_state=42): 29.80939874830973\n",
      "Test MPE for Lasso(alpha=0.1, random_state=42): 0.438066450571489\n",
      "\n",
      "Test MSE for ElasticNet(alpha=0.1, l1_ratio=1.0): 1539.831647666405\n",
      "Test MAE for ElasticNet(alpha=0.1, l1_ratio=1.0): 29.80939874830973\n",
      "Test MPE for ElasticNet(alpha=0.1, l1_ratio=1.0): 0.438066450571489\n",
      "\n",
      "Test MSE for DecisionTreeRegressor(random_state=42): 1333.3483007341024\n",
      "Test MAE for DecisionTreeRegressor(random_state=42): 26.932463525894462\n",
      "Test MPE for DecisionTreeRegressor(random_state=42): 0.3928439699155401\n",
      "\n",
      "Test MSE for RandomForestRegressor(max_depth=10, n_estimators=200, random_state=42): 1333.3052441897612\n",
      "Test MAE for RandomForestRegressor(max_depth=10, n_estimators=200, random_state=42): 26.93510579699631\n",
      "Test MPE for RandomForestRegressor(max_depth=10, n_estimators=200, random_state=42): 0.39300804134972944\n",
      "\n",
      "Test MSE for SVR(C=10, epsilon=0.01): 1462.555713854313\n",
      "Test MAE for SVR(C=10, epsilon=0.01): 26.174269062375306\n",
      "Test MPE for SVR(C=10, epsilon=0.01): 0.34020642485235963\n",
      "\n",
      "Test MSE for GradientBoostingRegressor(n_estimators=50, random_state=42): 1333.8285360893851\n",
      "Test MAE for GradientBoostingRegressor(n_estimators=50, random_state=42): 26.92669549485448\n",
      "Test MPE for GradientBoostingRegressor(n_estimators=50, random_state=42): 0.3928871257291931\n",
      "\n",
      "Test MSE for KNeighborsRegressor(n_neighbors=9): 1523.9438616683453\n",
      "Test MAE for KNeighborsRegressor(n_neighbors=9): 29.708007281052378\n",
      "Test MPE for KNeighborsRegressor(n_neighbors=9): 0.45002133599678973\n",
      "\n",
      "Test MSE for AdaBoostRegressor(learning_rate=0.01, random_state=42): 1368.1590419257598\n",
      "Test MAE for AdaBoostRegressor(learning_rate=0.01, random_state=42): 27.499963120789303\n",
      "Test MPE for AdaBoostRegressor(learning_rate=0.01, random_state=42): 0.40411251825866634\n",
      "\n",
      "Test MSE for XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=42, ...): 1333.318108134569\n",
      "Test MAE for XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=42, ...): 26.92991175536388\n",
      "Test MPE for XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=42, ...): 0.3928033252212575\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33\n",
      "[LightGBM] [Info] Number of data points in the train set: 13814, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 81.874548\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Test MSE for LGBMRegressor(max_depth=3, random_state=42): 1333.2908871705235\n",
      "Test MAE for LGBMRegressor(max_depth=3, random_state=42): 26.930557246919584\n",
      "Test MPE for LGBMRegressor(max_depth=3, random_state=42): 0.3928178265111579\n",
      "\n",
      "Test MSE for MLPRegressor(activation='tanh', hidden_layer_sizes=(50, 50), random_state=42,\n",
      "             solver='lbfgs'): 1336.8947457092497\n",
      "Test MAE for MLPRegressor(activation='tanh', hidden_layer_sizes=(50, 50), random_state=42,\n",
      "             solver='lbfgs'): 26.92007102647699\n",
      "Test MPE for MLPRegressor(activation='tanh', hidden_layer_sizes=(50, 50), random_state=42,\n",
      "             solver='lbfgs'): 0.3922549934391443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n",
    "# Ppredict with best params on X_test, y_test\n",
    "for model_detail in models:\n",
    "    model = models[model_detail]['model']\n",
    "    model.set_params(**best_params[model_detail])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mpe = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    print(f\"Test MSE for {models[model_detail]['model']}: {mse}\")\n",
    "    print(f\"Test MAE for {models[model_detail]['model']}: {mae}\")\n",
    "    print(f\"Test MPE for {models[model_detail]['model']}: {mpe}\")\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
